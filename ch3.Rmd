---
title: "概率与信息论"
author: "冯裕祺"
date: "2020/7/13"
output: ioslides_presentation
---
## 概率论和信息论

  本章讨论概率论和信息论。  
  概率论是用来表示不确定事件即随机事件的数学基础。不仅提出了量化不确定事件即随机事件，也提供了用于推导出新的随机事件的声明。  
  在人工智能领域，概率论的主要用途如下：  
- 概率法则告诉AI系统怎么推理,由此设计算法来计算或者估算由概率论推导出的表达式。  

- 从概率和统计从理论上分析我们提出的AI系统的行为。

## 3.1 为什么要使用概率论？


  计算机科学的许多处理的问题大部分都是完全确定且是必然事件。所以许多计算机科学家和软件工程师是在一个相对确定的环境中工作，但是机器学习环境中会处理很多不确定性事件。因此会用到很多概率论的知识.  

  几乎所有的人工智能活动都需要在不确定性的情况。
    
    
 
  不确定性有三种可能的来源：  

- 被建模系统内在的随机性。*例如纸牌游戏* 


- 对系统的观测是不完全的。*例如黑箱摸球模型，结果是确定的，但是从选手的角度来说结果是不确定的*  
  
  
- 建模时的不完全。    

## 

  例如，“基本上所有鸟儿都会飞”这个简单的规则我们人描述起来是很简单的。  
  但是如果我们想让计算机理解这一规则就需要加上很多定语。  
  “除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的鸟，包括鸵鸟(ostrich)、等不会飞的鸟类……以外，鸟儿会飞”。  
这样的规则是很难应用的，并且很容易失效。
  
  
  **我们需要一种对随机事件的的不确定性进行量化表达和推理的方法，概率论可以实现这一目的，但是概率论不能提供我们在人工智能领域所需要的所有的工具。**

---

  概率论最初的发展是为了了解不确定事件发生的频率。  
  我们可以从扑克牌游戏中抽出特定组合的的牌的事件中看出概率论是如何使用的。  
  当我们说一个结果发生的概率为$p$，这意味着如果我们重复实验无限次，有$p$的比率可能会导致这样的结果。但是这样的推理似乎不适用于不可重复的命题。  

---

  例如 一个医生诊断病人，并且说该病人患流感的概率为40%，这与扑克牌游戏的例子是完全不同的。  
  因为我们不能有无穷多个病人来重复这个患病的过程，也没有可能去相信不同的病人在不同的条件下会出现相同的症状。
  **总而言之，这种情况下我们无法进行重复无限次的实验来验证这个频率。**

  在医生诊断病人的例子中，我们用概率来表示一种**信任度（*degree of  belief*)**,1代表肯定患者患病，0代表肯定病人没有患病。  
  之前的纸牌游戏中的例子，概率和频率相互联系，被称为*频率派概率（frequentist probability)*;而医生诊断病人的的例子中我们将其转化为确定性水平，被称为*贝叶斯概率（Bayesian porbability)*

## 3.2 随机变量

  **随机变量（*random variable*)**是可以随机地抽取不同的值的变量。通常使用普通小写字母表示随机变量本身，用手写体的小写字母表示随机变量的具体的值。例如$x_1$,$x_2$都是随机变量$x$可能的取值。对于向量型变量，我们将随机变量写成X,它的一个可能取值为$x$。  
  从随机变量本身而言，一个随机变量只是对总体的可能的状态的描述,必须有一个概率分布来指定总体在每个状态的可能性。  
随机变量可以是离散或者连续的。

## 3.3概率分布
***概率分布（probability distribution)***用来描述随机变量或者一群随机变量在每一个可能取到的状态（即x的值）的状态的可能性的大小（y的值）。根据随机变量是离散型还是连续型来描述概率分布。  


## 3.3.1 离散型变量和概率质量函数
离散型随机变量的概率分布可以用***概率质量函数（probability mass funcition,PMF)***来描述。通常使用大写字母$P$来表示概率质量函数。通常每一个随机变量都会有一个不同的概率质量函数，并且要根据具体的随机变量来推断$PMF$,不能根据函数的名称来推断。例如$P(x)$,和$P(y)$通常是不一样的。
概率质量函数能够将每个随机变量的具体的取值与其在该状态的概率相对应。如$P(x=0.5)=1$表示当$x=0.5$的概率为1。是必然事件。  
有时会定义也一个随机变量，用~号来说明其遵循的分布：$x\sim P(x)$。
概率质量函数可以同时作用于多个随机变量。这种有多个随机变量的概率分布被称为联合概率分布。 $P(x = x, y = y)$表示$x = x$和$y = y$同时发生的概率。我们也可以简写为$P(x, y)$。

---
如果一个函数$P$是随机变量 $x$的PMF，必须满足下面这几个条件：   

- $P$的定义域必须是$x$所有可能状态的集合。  

- $\forall x \in x, 0\le P(x)\le 1.$不可能发生的事件概率为0，并且不存在比这概率更低的状态。 类似的，能够确保一定发生的事件概率为1，而且不存在比这概率更高的状态。  

- $\sum_{x \in x} P(x) = 1.$我们把这条性质称之为归一化的。   如果没有这条性质，当我们计算很多事件其中之一发生的概率时可能会得到大于1的概率。  
  
## 
例如，考虑一个离散型随机变量 $x$有$k$个不同的状态。 我们可以假设$x$是**均匀分布的（也就是将它的每个状态视为等可能的）**，通过将它的PMF设为：
$$
\begin{equation}
P(x = x_i) = \frac{1}{k}
\end{equation}
$$
一般对于所有$i$都成立。并且能满足上述三个条件。在此我们画出该PMF的图像，进一步理解PMF的意义。
  


##    

```{r echo=FALSE}
x<-runif(100,min = 0,max = 1)
plot(x,type = "b",col="#f0932b",ylab = "Mass")
legend("topright", c("X~1/k"), col = c("#f0932b"), lty = c(1),text.font = 12)
```  
  
  上图是均匀分布模拟100次的画出的概率质量函数图,将均匀分布的区间设为（0，1）。如果重复无限多次实验可以发现，其在纵轴的频率基本是相等的，也就是意味着均匀分布的概率是等可能的。
  
 
## 3.3.2 连续型变量和概率密度函数
当研究对象为连续性变量时，我们用***概率密度函数（probability destiny function)来描述概率分分布。如果一个函数$p$是概率密度函数，必须满足以下条件：   

- $p$的定义域必须是$x$所有可能状态的集合。  

- $\forall x \in x, p(x)\ge 0.$注意，我们并不要求$p(x)\le 1$。  

- $\int p(x) dx = 1.$  

概率密度函数 $p(x)$并没有直接对特定的状态给出概率，相对的，它给出了落在面积为$\delta x$ 的无限小的区域内的概率为$p(x)\delta x$。
这里我们通过正态分布来举例，连续性随机变量的概率密度函数。  
   
## 


```{r echo=FALSE}
plot(dnorm, -4, 4,main = "标准正态分布",ylab = "desnity")
```
  
  如上图所示，为标准正态分布的概率密度图，如果我们想要知道随机变量$x$在区间$[-2,2]$的概率。即为$\int_{[-2,2]} N(0,1)dx$。即为在图像中对-2到2这一段进行积分。

## 

```{r echo=FALSE}
plot(dnorm, -4, 4,ylab = "desnity",main="标准正态分布")
abline(v=c(-2,2),col="red")
```
   
从上图画出了积分区域即为(-2,2)。

## 3.4  边际概率
  有时我们知道了一组变量的联合概率分布，但想要了解其中一个具体的随机变量的概率分布。定义在子集上面的概率分布被称为***边际概率分布（marginal probability distribution)***。  
  假设有离散型随机变量$x$和$y$，并且我们知道$P(x,y)$。可以根据下面的求和法则来计算$P(X)$。$$\forall x \in x, P(x = x) = \sum_y P(x = x, y = y)$$
  对于连续型变量，我们需要用积分来求边际分布函数。$$p(x) = \int p(x, y)dy$$  
  当我们想要知道x的边际分布函数时，在y的区间上对$p(x,y）$对y积分即可。
  
## 3.5条件概率  

条件概率是十分重要的部分，在研究具体问题时，我们会想知道一件具体的事情发生的概率。但是真实世界中各种事物的联系是很精密的，所以要用条件概率来探究。我们将给定$x = x$，$y = y$发生的条件概率记为$P(y = y\mid x =x)$。可以通过下面的公式计算$$P(y = y\mid x = x) = \frac{P(y = y, x = x)}{P(x = x)} $$这个公式的意义是在$x$发生的情况下，$y$发生的概率。并且要求$P(x = x)>0$,因为条件概率不能计算在一个不可能事件发生的条件下的条件概率。  

##  
我们需要注意的是，不能将条件概率和计算采取某个动作之后会引起什么改变相混淆。如果某个人说德语，那么他是德国人的**条件概率**是非常高的。但是如果随机选择一个会说德语的人，他的国籍并不会因此而**改变**。   
我们将计算一个动作之后的影响称为***干预查询(intervention query)***。干预查询属于***因果模型(causal modeling)***的范畴，本书不做讨论。这里个人感觉和格兰杰因果检验有点像，***条件概率***是在给定一个有关联的事件的发生概率之后求出另一个本已经存在的事件的发生概率。而***干预查询***的本意是进行一个动作或者改变后，引起另一件事物的改变。

## 3.6 条件概率的链式法则
 任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式：$$P(x^{(1)}, \ldots, x^{(n)}) = P(x^{(1)}) \Pi_{i=2}^n P(x^{(i)} \mid x^{(1)}, \ldots, x^{(i-1)})$$
 这个规则被称为概率的***链式法则(chain rule)***或者***乘法法则(product rule)***。该定理可以由条件概率的公式推出：$$P(a, b, c) = P(a \mid b, c) P(b, c)$$
$$P(b, c) = P(b \mid c) P(c)$$
$$P(a, b, c) = P(a \mid b, c) P(b \mid c) P(c).$$

## 3.7独立性和条件独立性
  如果两个随机变量$x$和$y$，他们不相关，即没有相关性。并且它们的概率分布式可以表达为两个因子的乘积形式，其中式子只包含$x$和$y$。我们就称这两个随机变量是相互独立的。$$\forall x \in x, y \in y, p(x = x, y = y) = p(x = x)p(y = y).$$  
  如果关于$x$和$y$的条件概率分布对于$z$的每一个值都可以写成乘积的形式，那么这两个随机变量$x$和$y$在给定随机变量$z$时是**条件独立**的：$$\forall x \in x, y \in y, z \in z, p( x=x, y=y \mid z=z) =p(x = x \mid z = z) p(y = y \mid z = z).$$  
  我们可以用简单的形式来表示独立性和条件独立性：$x \bot y$表示$x$和$y$相互独立。$x \bot y \mid z$表示在给定条件$z$时，$x$和$y$相互独立。  
  
## 3.8期望、方差和协方差  
函数$f(x)$关于某分布$P(x)$的期望或者期望值是指，当$x$由$P$产生，$f$作用于$x$时，$f(x)$的平均值。 对于离散型随机变量，这可以通过求和得到：$$E_{x \sim P}[f(x)] = \sum_x P(x)f(x)$$
对于连续型随机变量可以通过积分得到：$$E_{x \sim p}[f(x)] = \int p(x)f(x)dx$$  

##  

当概率分布在上下文中指明时，我们可以只写出期望作用的随机变量的名称来进行简化，例如$E_{x}[f(x)]$。 如果期望作用的随机变量也很明确，我们可以完全不写脚标，就像$E[f(x)]$。 默认地，我们假设$E[\cdot]$表示对方括号内的所有随机变量的值求平均。 类似的，当没有歧义时，我们还可以省略方括号。
期望是线性的，例如:$$E_{x}[\alpha f(x) + \beta g(x)] = \alpha E_{x}[f(x)] + \beta E_{x}[g(x)]$$  
其中$\alpha$和$\beta$不依赖于$x$。
  
### 方差
**方差**衡量的是当我们对$x$依据它的概率分布进行采样时，随机变量 $x$的函数值会呈现多大的差异：$$Var(f(x)) = E \left [(f(x) - E[f(x)])^2 \right]$$
  
##  
**协方差**在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：$$Cov(f(x), g(y)) = E[ ( f(x)-E[f(x)] )( g(y)-E[g(y)] ) ]$$。
协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很远。 如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。 如果协方差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值，反之亦然。 其他的衡量指标如***相关系数***将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。  

协方差和相关性是有联系的，但实际上是不同的概念。它们是有联系的，因为两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的。 然而，独立性又是和协方差完全不同的性质。两个变量如果协方差为零，它们之间一定没有线性关系。
  
##  
独立性比零协方差的要求更强，因为独立性还排除了非线性的关系。 两个变量相互依赖但具有零协方差是可能的。 例如，假设我们首先从区间$[-1, 1]$上的均匀分布中采样出一个实数$x$。 然后我们对一个随机变量 $s$进行采样。 $s$以$\frac{1}{2}$的概率值为1，否则为-1。我们可以通过令$y=sx$来生成一个随机变量$y$。显然，$x$和$y$不是相互独立的，因为$x$完全决定了$y$的尺度。 然而，$Cov(x,y)=0$。  
随机向量$x \in R^n$的**协方差矩阵**是一个$n\times n$的矩阵，并且满足:$$Cov(x)_{i,j} = Cov(x_i, x_j)$$  
协方差矩阵的对角元是方差：$$Cov(x_i, x_i) = Var(x_i).$$  

## 3.9 常用概率分布
### Bernoulli（伯努利）分布
  Bernoulli分布是单个二值随机变量的分布。 它由单个参数$\phi \in [0, 1]$控制，$\phi$给出了随机变量等于1的概率。 它具有如下的一些性质：  
 
- $$P(x =1) = \phi $$  
  
- $$P(x =0) = 1-\phi$$ 

- $$P(x = x) = \phi^x (1-\phi)^{1-x}$$

- $$E_{x}[x] = \phi$$  
  
- $$Var_{x}(x) = \phi(1-\phi)$$
 
## 
伯努利分布可以计算出较为简单的概率模型的概率。如投掷硬币，投掷8次，七次为正面的概率可以通过伯努利分布算出。
![伯努利分布概率质量函数](https://math.jianshu.com/math?formula=P(x)%3Dp%5Ex(1-p)%5E%7B1-x%7D%3D%20%5Cbegin%7Bcases%7D%20p%20%26%20%5Ctext%7Bif%20%24x%24%3D1%7D%20%5C%5C%20q%20%26%20%5Ctext%7Bif%20%24x%24%3D0%7D%20%5Cend%7Bcases%7D)  

![](https://math.jianshu.com/math?formula=E(x)%3D%5Csum%20xP(x)%3D0%5Ctimes%20q%20%2B%201%5Ctimes%20p%3Dp)  

![](https://math.jianshu.com/math?formula=Var(x)%3DE%5B(x-E(x))%5E2%5D%3D%20%5Csum%20(x-p)%5E2P(x)%20%3Dpq)   
  
## 3.9.2Multinouli分布
  
  ***Multinouli分布***又称***范畴分布***，是 Bernoulli分布从两个取值状态到多个取值状态的扩展。具体来说，mutinoulli分布是指在具有k个不同状态的单个离散型随机变量上的分布，其中k是一个有限值，且满足k个状态的概率之和为1。例如：掷骰子游戏中，每次投掷可能出现6种结果，对应6个状态，每个状态的概率均为六分之一。  
  Multinoulli分布由向量$p\in[0,1]^{k-1}$参数化，其中每一个分量$p_i$表示第$i$个状态的概率。 最后的第$k$个状态的概率可以通过$1-1^\top p$给出。 注意我们必须限制$1^\top p \le 1$。 Multinoulli分布经常用来表示对象分类的分布，所以我们很少假设状态1具有数值1之类的。 因此，我们通常不需要去计算Multinoulli分布的随机变量的期望和方差。  
在机器学习中，该分布常用于分类，表示分类算法的k个类，而类之间是没有可比计算性的。  
 
## 3.9.3正态分布
最常用的分布就是正态分布，也称为高斯分布：$$N(x; \mu, \sigma^2) = \sqrt{\frac{1}{2\pi \sigma^2}} \exp \left ( -\frac{1}{2\sigma^2} (x-\mu)^2 \right )$$
正态分布由两个参数控制，$\mu \in R$和$\sigma \in(0,\infty)$。参数$\mu$给出了中心峰值的坐标，这也是分布的均值：$E[x] = \mu$。分布的标准差用$\sigma$表示，方差用$\sigma^2$表示。
  
## 

```{r echo=FALSE}
set.seed(1)
x <- seq(-10,15,length.out = 1000)
# 计算X~N(-2,1)
y1 <- dnorm(x, -2,1)
# 计算X~N(2,1)
y2 <- dnorm(x, 2, 1)
# 计算X~N(2,4)
y3 <- dnorm(x, 2, 2)
# 绘图
plot(x, y1, type = "l", col="#f0932b", ylab = "Density", lwd=2, xlim = c(-8,10))
lines(x, y2, lwd=2, col="#4834d4")
lines(x, y3, lwd=2, col="#95afc0")
legend("topright", c("X~N(-2,1)", "X~N(2,1)", "X~N(2,4)"), col = c("#f0932b", "#4834d4", "#95afc0"), lty = c(1),text.font = 12)

```

从上图可以看出,正态分布的均值决定了对称轴，方差决定了图像的宽度。  
 
##
采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选择，其中有两个原因。  
  
- 我们想要建模的很多分布的真实情况是比较接近正态分布的。***中心极限定理***说明很多独立随机变量的和近似服从正态分布。 这意味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可以被分解成一些更结构化的部分。  
  
- 在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。 充分利用和证明这个想法需要更多的数学工具，我们推迟到19.4.2进行讲解.

##   
正态分布可以推广到$R^n$空间，这种情况下被称为多维正态分布。 它的参数是一个正定对称矩阵$\Sigma$:$$N(x; \mu, \Sigma) = \sqrt{ \frac{1}{ (2\pi)^n \det(\Sigma)}}  \exp \left ( -\frac{1}{2} (x-\mu)^\top \Sigma^{-1} (x- \mu) \right)$$
参数$\mu$仍然表示分布的均值，只不过现在是向量值。 参数$\Sigma$给出了分布的协方差矩阵。  
  
## 3.9.4指数分布和Laplace分布
在深度学习中，我们经常会需要一个在$x=0$点处取得边界点(sharppoint)的分布。为了实现这一目的，我们可以使用指数分布：$$p(x; \lambda) = \lambda 1_{x\ge 0} \exp(-\lambda x)$$。

##

```{r echo=FALSE}
curve(dexp(x,rate=1),xlim=c(0,5))
```
上图为指数分布的概率密度函数，设定参数$\lambda = 1$。可以看出在$x=0$处取得最大值。指数分布使用指示函数(indicator function)$1_{x\ge 0}$来使得当$x$取负值时的概率为零。  

## 
一个联系紧密的概率分布是*Laplace分布*，允许我们在任意一点$\mu$处设置概率质量的峰值。
$$\text{Laplace}(x; \mu, \gamma) = \frac{1}{2\gamma} \exp \left( -\frac{|x-\mu|}{\gamma}  \right)$$
  
##  

<div align="center">
<img src="https://bkimg.cdn.bcebos.com/pic/d439b6003af33a87293da63acc5c10385343b5ba?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5"  height="400" width="400">
  
  
## 3.9.5Dirac分布和经验分布
在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通过Diracdelta函数$\delta(x)$定义概率密度函数来实现：$$p(x) = \delta(x-\mu)$$
它可以描述成一个在原点处无限高，无限窄的曲线，并且它的积分为 1。也就是说只在原点处取 +∞，而在其他各处取 0。
我们通常就将 δ 函数，理解为支集为原点的一个函数；或者 δ 函数定义为一个分布时，它实际就是对应于支撑集{0}的概率测度。  
  
##   
<div aligen = >
![](https://img-blog.csdn.net/20170313213054763?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveXVhbm1lbmd4aW5nbG9uZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

##  
Dirac delta函数被定义成在除了0以外的所有点的值都为0，但是积分为1。Diracdelta函数不像普通函数一样对$x$的每一个值都有一个实数值的输出，它是一种不同类型的数学对象，被称为广义函数，广义函数是依据积分性质定义的数学对象。我们可以把Dirac delta函数想成一系列函数的极限点，这一系列函数把除0以外的所有点的概率密度越变越小。
通过把$p(x)$定义成$\delta$函数左移$-\mu$个单位，我们得到了一个在$x=\mu$ 处具有无限窄也无限高的峰值的概率质量。
Dirac分布经常作为经验分布的一个组成部分出现：$$\hat{p}(x) = \frac{1}{m} \sum_{i=1}^m \delta(x - x^{(i)})$$
经验分布将概率密度$\frac{1}{m}$赋给$m$个点$x^{(1)}, \ldots, x^{(m)}$中的每一个，这些点是给定的数据集或者采样的集合。 只有在定义连续型随机变量的经验分布时，Dirac delta函数才是必要的。 对于离散型随机变量，情况更加简单：经验分布可以被定义成一个Multinoulli分布，对于每一个可能的输入，其概率可以简单地设为在训练集上那个输入值的经验频率。

## 3.9.6 分布的混合
通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组合方法是构造**混合分布**。混合分布由一些组件(component)分布构成。 每次实验，样本是由哪个组件分布产生的取决于从一个Multinoulli分布中采样的结果：$$P(x) = \sum_i P(c = i) P(x \mid c = i)$$   
简而言之就是在概率与统计中，如果我们有一个包含多个随机变量的随机变量集合，再基于该集合生成一个新的随机变量，则该随机变量的分布称为***混合分布(mixture distribution)***。
 
##   

例如我们可以构造一个：  
  
- 40%来自$N~(2,8)$
  
- 20%来自柯西分布（25，2）
  
- 40%来自$N~(10,6)$

该分布为混合分布，分别从三个分布中抽样得出，并且我们可以得出他的概率密度函数并画出图像。  
  
##  

```{r echo=FALSE}
library(distr)

## Construct the distribution object.
myMix <- UnivarMixingDistribution(Norm(mean=2, sd=8), 
                                  Cauchy(location=25, scale=2),
                                  Norm(mean=10, sd=6),
                                  mixCoeff=c(0.4, 0.2, 0.4))
## ... and then a function for sampling random variates from it
rmyMix <- r(myMix)

## Sample a million random variates, and plot (part of) their histogram
x <- rmyMix(1e6)
hist(x[x>-100 & x<100], breaks=100, col="grey", main="混合分布频数分布图")
```
  
##  
```{r echo=FALSE}
plot(myMix, to.draw.arg="d",main="混合函数概率密度分布图") 
```
  
从上图可以看出该混合分布呈现双峰，这是因为两个正态分布的均值相差比较大，所以导致了双峰的状况。  

##  
  
混合模型使我们能够一瞥以后会用到的一个非常重要的概念——***潜变量（latent variable)***。  
比如，你可能不想买一件商品，但在不同场合看到了广告之后，最终决定买一件试式，当你看一次两次广告时没什么感觉，看多了就会有行动和决策了。这里的潜变量是我们购买衣服的意愿，一次两次还是0，过了一定量就是1了，概率是看不见的，但是可以测量，潜变量也只是一种解释性的模型，没有绝对真理。

  
##  

最常用的混合模型是***高斯混合模型(Gaussian Mixture Model)***。例如下图的分布如果建立单高斯分布模型，效果不会很好。
![](https://pic1.zhimg.com/80/e7a9afab7a1dbcdf4be951f8c14c950c_hd.png)
  
##  
 
于是，我们引入混合高斯模型（Gaussian Mixture Model，GMM）。高斯混合模型就是多个单高斯模型的和。它的表达能力十分强，任何分布都可以用GMM来表示。例如，在下面这个图中，彩色的线表示一个一个的单高斯模型，黑色的线是它们的和，一个高斯混合模型：
![](https://pic4.zhimg.com/80/d5219ab48fa3765b837a1b09689dc643_hd.png)

## 
***高斯混合模型***是概率密度的**万能近似器(universal approximator)**。基本上所有的平滑概率密度都可以用足够多组件的高斯混合模型以任意精度来逼近。
  
## 先验概率和后验概率
 在高斯混合模型中有两个重要的概念是**先验概率**和**后验概率**。  
 
- 先验概率是指根据以往经验和分析得到的概率,如全概率公式,它往往作为"由因求果"问题中的"因"出现。  
  
- 后验概率是指依据得到"结果"信息所计算出的最有可能是那种事件发生,如贝叶斯公式中的,是"执果寻因"问题中的"因"。  
  
这个比较抽象，我们可以用生活中的例子来更好地理解。

例如桌子上有一杯白水和一袋糖，你喝了一口水，发现水是甜的。你觉得水里加了糖的概率是多大？你说：有80%可能性加了糖。这就是一次后验概率。这里由结果“水是甜的”，推出了加了糖的概率是80%。这就是一次后验概率的过程，由结果去寻找原因。  
  
##   

先验概率的例子很简单。  
比如抛硬币，我们都认为正面朝上的概率是0.5，这就是一种先验概率，在抛硬币前，我们只有常识。这个时候事情还没发生，我们进行概率判断。所谓的先验概率是对事情发生可能性猜测的数学表示。  



##   
<div align="center">
<img src="C:/Users/YuQiFeng/Desktop/1595237887(1).jpg"  height="300" width="500">  

<article>
  
  <p>在这个示例中，有三个组件。从左到右，第一个组件具有各向同性的协方差矩阵，这意味着它在每个方向上具有相同的方差。 第二个组件具有对角的协方差矩阵，这意味着它可以沿着每个轴的对齐方向单独控制方差。
该示例中，沿着$x2$轴的方差要比沿着$x1$轴的方差大。第三个组件具有满秩的协方差矩阵，使它能够沿着任意基的方向单独地控制方差。</p>
</article>
